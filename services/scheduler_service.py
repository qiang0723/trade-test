"""
Scheduler Service - å®šæ—¶ä»»åŠ¡æœåŠ¡

èŒè´£ï¼š
1. ç®¡ç†å®šæ—¶ä»»åŠ¡ï¼ˆå®šæœŸå†³ç­–æ›´æ–°ã€æ•°æ®æ¸…ç†ï¼‰
2. å°è£…APScheduleré€»è¾‘
"""

import logging
from typing import Optional

logger = logging.getLogger(__name__)

# å¯é€‰ä¾èµ–ï¼šAPScheduler
try:
    from apscheduler.schedulers.background import BackgroundScheduler
    APSCHEDULER_AVAILABLE = True
except ImportError:
    APSCHEDULER_AVAILABLE = False
    logger.warning("apscheduler not installed, auto cleanup disabled")


class SchedulerService:
    """å®šæ—¶ä»»åŠ¡æœåŠ¡"""
    
    def __init__(self, advisory_engine, l1_db, binance_fetcher, config):
        """
        åˆå§‹åŒ–å®šæ—¶ä»»åŠ¡æœåŠ¡
        
        Args:
            advisory_engine: L1AdvisoryEngineå®ä¾‹
            l1_db: L1Databaseå®ä¾‹
            binance_fetcher: BinanceDataFetcherå®ä¾‹
            config: é…ç½®å­—å…¸
        """
        self.advisory_engine = advisory_engine
        self.l1_db = l1_db
        self.binance_fetcher = binance_fetcher
        self.config = config
        self.scheduler = None
    
    def start(self) -> Optional[object]:
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        if not APSCHEDULER_AVAILABLE:
            logger.warning("APScheduler not available, skipping scheduler")
            return None
        
        try:
            periodic_config = self.config.get('periodic_update', {})
            
            if not periodic_config.get('enabled', True):
                logger.warning("Periodic advisory update is disabled in config")
                return None
            
            self.scheduler = BackgroundScheduler()
            
            # ä»»åŠ¡1: å®šæ—¶è‡ªåŠ¨è·å–å†³ç­–å¹¶ä¿å­˜
            interval_minutes = periodic_config.get('interval_minutes', 1)
            self.scheduler.add_job(
                func=self._periodic_advisory_update,
                trigger='interval',
                minutes=interval_minutes,
                id='periodic_advisory',
                name=f'Periodic L1 advisory update (every {interval_minutes} minute(s))',
                max_instances=1,
                next_run_time=None
            )
            
            # ä»»åŠ¡2: å®šæœŸæ¸…ç†æ—§æ•°æ®
            retention_config = self.config.get('data_retention', {})
            cleanup_interval = retention_config.get('cleanup_interval_hours', 6)
            self.scheduler.add_job(
                func=self._cleanup_old_records_job,
                trigger='cron',
                hour=f'*/{cleanup_interval}',
                minute=0,
                id='cleanup_old_records',
                name=f'Cleanup old L1 advisory records (every {cleanup_interval}h)'
            )
            
            self.scheduler.start()
            logger.info("â° Scheduler started:")
            logger.info(f"  - Periodic advisory update: Every {interval_minutes} minute(s)")
            logger.info(f"  - Cleanup old records: Every {cleanup_interval} hours")
            
            return self.scheduler
        
        except Exception as e:
            logger.error(f"Error starting scheduler: {e}")
            return None
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if self.scheduler:
            self.scheduler.shutdown()
            logger.info("Scheduler stopped")
    
    def _periodic_advisory_update(self):
        """å®šæ—¶æ›´æ–°ä»»åŠ¡ï¼šæ¯åˆ†é’Ÿè‡ªåŠ¨è·å–å¸‚åœºæ•°æ®å¹¶ç”Ÿæˆå†³ç­–"""
        try:
            logger.info("â° Running periodic advisory update...")
            
            symbols = self.config.get('symbols', ['BTCUSDT'])
            market_type = self.config.get('periodic_update', {}).get('market_type', 'futures')
            
            error_config = self.config.get('error_handling', {})
            max_retries = error_config.get('max_retries', 3)
            retry_delay = error_config.get('retry_delay_seconds', 5)
            continue_on_error = error_config.get('continue_on_error', True)
            
            if not symbols:
                logger.warning("No symbols configured for monitoring")
                return
            
            for symbol in symbols:
                market_data = None
                last_error = None
                
                # é‡è¯•é€»è¾‘
                for attempt in range(max_retries):
                    try:
                        market_data = self.binance_fetcher.fetch_market_data(symbol, market_type=market_type)
                        
                        if market_data:
                            break
                        else:
                            last_error = f"No market data returned for {symbol}"
                            if attempt < max_retries - 1:
                                logger.warning(f"Attempt {attempt + 1}/{max_retries} failed for {symbol}, retrying...")
                                import time
                                time.sleep(retry_delay)
                        
                    except Exception as e:
                        last_error = str(e)
                        if attempt < max_retries - 1:
                            logger.warning(f"Attempt {attempt + 1}/{max_retries} failed for {symbol}: {e}")
                            import time
                            time.sleep(retry_delay)
                        else:
                            logger.error(f"All {max_retries} attempts failed for {symbol}: {e}", exc_info=True)
                
                if not market_data:
                    logger.error(f"âŒ Failed to fetch {symbol} after {max_retries} attempts: {last_error}")
                    if continue_on_error:
                        continue
                    else:
                        break
                
                # ç”ŸæˆL1å†³ç­–å¹¶ä¿å­˜
                try:
                    result = self.advisory_engine.on_new_tick(symbol, market_data)
                    advisory_id = self.l1_db.save_advisory_result(symbol, result)
                    
                    if hasattr(self.advisory_engine, 'last_pipeline_steps'):
                        self.l1_db.save_pipeline_steps(advisory_id, symbol, self.advisory_engine.last_pipeline_steps)
                    
                    logger.info(
                        f"âœ… Periodic update saved: {symbol} â†’ {result.decision.value} "
                        f"(confidence: {result.confidence.value}, executable: {result.executable})"
                    )
                
                except Exception as e:
                    logger.error(f"Error processing decision for {symbol}: {e}", exc_info=True)
                    if continue_on_error:
                        continue
                    else:
                        break
        
        except Exception as e:
            logger.error(f"Error in periodic_advisory_update: {e}", exc_info=True)
    
    def _cleanup_old_records_job(self):
        """å®šæ—¶æ¸…ç†æ—§è®°å½•çš„ä»»åŠ¡ï¼ˆä¿ç•™24å°æ—¶ï¼‰"""
        try:
            deleted = self.l1_db.cleanup_old_records(days=1)
            logger.info(f"ğŸ—‘ï¸  Auto cleanup completed: {deleted} old records deleted")
        except Exception as e:
            logger.error(f"Error in auto cleanup job: {e}", exc_info=True)
