"""
L1 Advisory Layer - Flask Web Application

Flaskåç«¯åº”ç”¨ï¼Œæä¾›ï¼š
1. L1 Advisoryå†³ç­–API
2. å†å²å†³ç­–æŸ¥è¯¢
3. Web UIç•Œé¢
4. é˜ˆå€¼é…ç½®ç®¡ç†
"""

from flask import Flask, jsonify, request, render_template
from flask_cors import CORS
from market_state_machine_l1 import L1AdvisoryEngine
from database_l1 import L1Database
from models.reason_tags import REASON_TAG_EXPLANATIONS, get_reason_tag_category
from binance_data_fetcher import get_fetcher
import logging
from datetime import datetime
import yaml
import os

# âš ï¸  ä¿®å¤ï¼šå…ˆé…ç½®æ—¥å¿—ï¼Œå†ä½¿ç”¨loggerï¼ˆPR-M å»ºè®®Aç›¸å…³ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯é€‰ï¼šWatchdogæ–‡ä»¶ç›‘æ§ï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    logger.warning("watchdog not installed, config hot reload disabled. Install: pip install watchdog")

# å¯é€‰ï¼šAPSchedulerå®šæ—¶ä»»åŠ¡ï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
try:
    from apscheduler.schedulers.background import BackgroundScheduler
    APSCHEDULER_AVAILABLE = True
except ImportError:
    APSCHEDULER_AVAILABLE = False
    logger.warning("apscheduler not installed, auto cleanup disabled. Install: pip install apscheduler")

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# åˆå§‹åŒ–L1å¼•æ“ã€æ•°æ®åº“å’ŒBinanceæ•°æ®è·å–å™¨
advisory_engine = L1AdvisoryEngine()
l1_db = L1Database()
binance_fetcher = get_fetcher()

logger.info("Flask app initialized with L1 Advisory Engine and Binance Fetcher")


# ========================================
# é…ç½®çƒ­æ›´æ–°ï¼ˆWatchdogï¼‰
# ========================================

class ConfigReloader(FileSystemEventHandler):
    """é…ç½®æ–‡ä»¶ç›‘æ§å’Œçƒ­æ›´æ–°"""
    
    def __init__(self, engine: L1AdvisoryEngine):
        self.engine = engine
        self.config_path = os.path.join(
            os.path.dirname(__file__), 
            'config', 
            'l1_thresholds.yaml'
        )
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹æ—¶è§¦å‘"""
        if not event.is_directory and event.src_path.endswith('l1_thresholds.yaml'):
            logger.info(f"Config file modified: {event.src_path}")
            self._reload_config()
    
    def _reload_config(self):
        """é‡è½½é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                new_config = yaml.safe_load(f)
            
            # æ‰å¹³åŒ–é˜ˆå€¼
            new_thresholds = self.engine._flatten_thresholds(new_config)
            
            # æ›´æ–°å¼•æ“é˜ˆå€¼
            self.engine.update_thresholds(new_thresholds)
            
            logger.info(f"âœ… Config reloaded successfully: {len(new_thresholds)} thresholds updated")
            
        except Exception as e:
            logger.error(f"âŒ Error reloading config: {e}", exc_info=True)


def start_config_watcher():
    """å¯åŠ¨é…ç½®æ–‡ä»¶ç›‘æ§"""
    if not WATCHDOG_AVAILABLE:
        logger.warning("Watchdog not available, skipping config file watcher")
        return None
    
    try:
        config_dir = os.path.join(os.path.dirname(__file__), 'config')
        
        if not os.path.exists(config_dir):
            logger.warning(f"Config directory not found: {config_dir}")
            return None
        
        observer = Observer()
        event_handler = ConfigReloader(advisory_engine)
        observer.schedule(event_handler, config_dir, recursive=False)
        observer.start()
        
        logger.info(f"ğŸ“ Config file watcher started: {config_dir}")
        return observer
    
    except Exception as e:
        logger.error(f"Error starting config watcher: {e}")
        return None


# å¯åŠ¨é…ç½®ç›‘æ§
config_observer = start_config_watcher()


# ========================================
# å®šæ—¶æ¸…ç†æ•°æ®åº“ï¼ˆAPSchedulerï¼‰
# ========================================

def cleanup_old_records_job():
    """å®šæ—¶æ¸…ç†æ—§è®°å½•çš„ä»»åŠ¡ï¼ˆä¿ç•™24å°æ—¶ï¼‰"""
    try:
        deleted = l1_db.cleanup_old_records(days=1)  # åªä¿ç•™1å¤©ï¼ˆ24å°æ—¶ï¼‰
        logger.info(f"ğŸ—‘ï¸  Auto cleanup completed: {deleted} old records deleted")
    except Exception as e:
        logger.error(f"Error in auto cleanup job: {e}", exc_info=True)


def load_monitored_symbols():
    """åŠ è½½ç›‘æ§çš„äº¤æ˜“å¯¹é…ç½®"""
    try:
        config_path = os.path.join(
            os.path.dirname(__file__), 
            'config', 
            'monitored_symbols.yaml'
        )
        
        if not os.path.exists(config_path):
            logger.warning(f"Monitored symbols config not found: {config_path}, using default [BTCUSDT]")
            return {
                'periodic_update': {'enabled': True, 'interval_minutes': 1, 'market_type': 'futures'},
                'symbols': ['BTCUSDT']
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        logger.info(f"Loaded monitored symbols config: {len(config.get('symbols', []))} symbols")
        return config
    
    except Exception as e:
        logger.error(f"Error loading monitored symbols config: {e}")
        return {
            'periodic_update': {'enabled': True, 'interval_minutes': 1, 'market_type': 'futures'},
            'symbols': ['BTCUSDT']
        }


def periodic_advisory_update():
    """
    å®šæ—¶æ›´æ–°ä»»åŠ¡ï¼šæ¯åˆ†é’Ÿè‡ªåŠ¨è·å–å¸‚åœºæ•°æ®å¹¶ç”Ÿæˆå†³ç­–
    
    è¿™ç¡®ä¿å³ä½¿å‰ç«¯å…³é—­ï¼Œå†å²å†³ç­–è®°å½•ä¹Ÿä¼šæŒç»­æ›´æ–°ï¼Œä¸ä¼šå‡ºç°æ•°æ®ç¼ºå¤±
    
    åŒ…å«é‡è¯•æœºåˆ¶ï¼šåˆ©ç”¨é…ç½®ä¸­çš„ error_handling.max_retries å’Œ retry_delay_seconds
    """
    try:
        logger.info("â° Running periodic advisory update...")
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ç›‘æ§çš„äº¤æ˜“å¯¹å’Œé”™è¯¯å¤„ç†ç­–ç•¥
        config = load_monitored_symbols()
        symbols = config.get('symbols', ['BTCUSDT'])
        market_type = config.get('periodic_update', {}).get('market_type', 'futures')
        
        # è¯»å–é‡è¯•é…ç½®
        error_config = config.get('error_handling', {})
        max_retries = error_config.get('max_retries', 3)
        retry_delay = error_config.get('retry_delay_seconds', 5)
        continue_on_error = error_config.get('continue_on_error', True)
        
        if not symbols:
            logger.warning("No symbols configured for monitoring")
            return
        
        for symbol in symbols:
            market_data = None
            last_error = None
            
            # é‡è¯•é€»è¾‘
            for attempt in range(max_retries):
                try:
                    # 1. è·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
                    market_data = binance_fetcher.fetch_market_data(symbol, market_type=market_type)
                    
                    if market_data:
                        break  # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    else:
                        last_error = f"No market data returned for {symbol}"
                        if attempt < max_retries - 1:
                            logger.warning(f"Attempt {attempt + 1}/{max_retries} failed for {symbol}, retrying in {retry_delay}s...")
                            import time
                            time.sleep(retry_delay)
                        
                except Exception as e:
                    last_error = str(e)
                    if attempt < max_retries - 1:
                        logger.warning(f"Attempt {attempt + 1}/{max_retries} failed for {symbol}: {e}, retrying in {retry_delay}s...")
                        import time
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"All {max_retries} attempts failed for {symbol}: {e}", exc_info=True)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ•°æ®
            if not market_data:
                logger.error(f"âŒ Failed to fetch {symbol} after {max_retries} attempts: {last_error}")
                if continue_on_error:
                    continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªsymbol
                else:
                    break  # ä¸­æ–­æ•´ä¸ªä»»åŠ¡
            
            # 2. ç”ŸæˆL1å†³ç­–
            try:
                result = advisory_engine.on_new_tick(symbol, market_data)
                
                # 3. ä¿å­˜åˆ°æ•°æ®åº“
                advisory_id = l1_db.save_advisory_result(symbol, result)
                
                # 4. ä¿å­˜ç®¡é“æ­¥éª¤
                if hasattr(advisory_engine, 'last_pipeline_steps'):
                    l1_db.save_pipeline_steps(advisory_id, symbol, advisory_engine.last_pipeline_steps)
                
                logger.info(
                    f"âœ… Periodic update saved: {symbol} â†’ {result.decision.value} "
                    f"(confidence: {result.confidence.value}, executable: {result.executable})"
                )
            
            except Exception as e:
                logger.error(f"Error processing decision for {symbol}: {e}", exc_info=True)
                if continue_on_error:
                    continue
                else:
                    break
        
    except Exception as e:
        logger.error(f"Error in periodic_advisory_update: {e}", exc_info=True)


def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    if not APSCHEDULER_AVAILABLE:
        logger.warning("APScheduler not available, skipping auto cleanup and periodic updates")
        return None
    
    try:
        # åŠ è½½é…ç½®
        config = load_monitored_symbols()
        periodic_config = config.get('periodic_update', {})
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®šæ—¶æ›´æ–°
        if not periodic_config.get('enabled', True):
            logger.warning("Periodic advisory update is disabled in config")
            return None
        
        scheduler = BackgroundScheduler()
        
        # ä»»åŠ¡1: å®šæ—¶è‡ªåŠ¨è·å–å†³ç­–å¹¶ä¿å­˜ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
        interval_minutes = periodic_config.get('interval_minutes', 1)
        scheduler.add_job(
            func=periodic_advisory_update,
            trigger='interval',
            minutes=interval_minutes,
            id='periodic_advisory',
            name=f'Periodic L1 advisory update (every {interval_minutes} minute(s))',
            max_instances=1,  # é˜²æ­¢å¹¶å‘æ‰§è¡Œ
            next_run_time=None  # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡
        )
        
        # ä»»åŠ¡2: å®šæœŸæ¸…ç†æ—§æ•°æ®
        retention_config = config.get('data_retention', {})
        cleanup_interval = retention_config.get('cleanup_interval_hours', 6)
        scheduler.add_job(
            func=cleanup_old_records_job,
            trigger='cron',
            hour=f'*/{cleanup_interval}',
            minute=0,
            id='cleanup_old_records',
            name=f'Cleanup old L1 advisory records (every {cleanup_interval}h)'
        )
        
        scheduler.start()
        logger.info("â° Scheduler started:")
        logger.info(f"  - Periodic advisory update: Every {interval_minutes} minute(s)")
        logger.info(f"  - Cleanup old records: Every {cleanup_interval} hours")
        logger.info(f"  - Monitored symbols: {config.get('symbols', [])}")
        
        return scheduler
    
    except Exception as e:
        logger.error(f"Error starting scheduler: {e}")
        return None


# å¯åŠ¨å®šæ—¶ä»»åŠ¡
scheduler = start_scheduler()


# ========================================
# Webé¡µé¢è·¯ç”±
# ========================================

@app.route('/')
def index():
    """L1 Advisoryä¸»é¡µ"""
    return render_template('index_l1.html')


# ========================================
# APIè·¯ç”± - L1 Advisory
# ========================================

@app.route('/api/l1/advisory/<symbol>')
def get_advisory(symbol):
    """
    è·å–æŒ‡å®šå¸ç§çš„æœ€æ–°L1å†³ç­–
    
    GET /api/l1/advisory/BTC
    
    Response:
    {
      "success": true,
      "data": {
        "decision": "long",
        "confidence": "high",
        "market_regime": "trend",
        "system_state": "long_active",
        "risk_exposure_allowed": true,
        "trade_quality": "good",
        "reason_tags": ["strong_buy_pressure", "oi_growing"],
        "timestamp": "2026-01-20T15:30:45.123456"
      }
    }
    """
    try:
        logger.info(f"API request: /api/l1/advisory/{symbol}")
        
        # 1. è·å–å¸‚åœºæ•°æ®
        market_data_dict = fetch_market_data(symbol)
        
        if not market_data_dict:
            logger.warning(f"Failed to fetch market data for {symbol}")
            return jsonify({
                'success': False,
                'data': None,
                'message': f'Failed to fetch market data for {symbol}'
            }), 404
        
        # 2. L1å†³ç­–
        result = advisory_engine.on_new_tick(symbol, market_data_dict)
        
        # 3. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåŒ…å«pipeline stepsï¼ŒPR-007ï¼‰
        try:
            advisory_id = l1_db.save_advisory_result(symbol, result)
            # PR-007: ä¿å­˜pipelineæ‰§è¡Œæ­¥éª¤
            if advisory_engine.last_pipeline_steps:
                l1_db.save_pipeline_steps(advisory_id, symbol, advisory_engine.last_pipeline_steps)
        except Exception as e:
            logger.error(f"Error saving to database: {e}")
            # ä¸å½±å“APIè¿”å›
        
        # 4. è¿”å›ç»“æœ
        return jsonify({
            'success': True,
            'data': result.to_dict(),
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_advisory: {str(e)}', exc_info=True)
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


@app.route('/api/l1/history/<symbol>')
def get_history(symbol):
    """
    è·å–å†å²å†³ç­–
    
    GET /api/l1/history/BTC?hours=24&limit=1500
    
    Query Parameters:
    - hours: å›æº¯å°æ—¶æ•°ï¼ˆé»˜è®¤24ï¼‰
    - limit: è¿”å›æ¡æ•°ï¼ˆé»˜è®¤1500ï¼ŒæŒ‰1åˆ†é’Ÿä¸€æ¬¡ï¼Œ24å°æ—¶çº¦1440æ¬¡ï¼‰
    
    Response:
    {
      "success": true,
      "data": [
        {
          "decision": "long",
          "confidence": "high",
          "timestamp": "2026-01-20T15:30:45",
          "reason_tags": ["..."],
          ...
        },
        ...
      ]
    }
    """
    try:
        hours = int(request.args.get('hours', 24))
        limit = int(request.args.get('limit', 1500))
        
        logger.info(f"API request: /api/l1/history/{symbol}?hours={hours}&limit={limit}")
        
        history = l1_db.get_history_advisory(symbol, hours, limit)
        
        return jsonify({
            'success': True,
            'data': history,
            'count': len(history),
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_history: {str(e)}', exc_info=True)
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


@app.route('/api/l1/stats/<symbol>')
def get_stats(symbol):
    """
    è·å–å†³ç­–ç»Ÿè®¡ä¿¡æ¯
    
    GET /api/l1/stats/BTC?hours=24
    
    Response:
    {
      "success": true,
      "data": {
        "total": 100,
        "long": 30,
        "short": 20,
        "no_trade": 50,
        "high_confidence": 15,
        ...
      }
    }
    """
    try:
        hours = int(request.args.get('hours', 24))
        
        logger.info(f"API request: /api/l1/stats/{symbol}?hours={hours}")
        
        stats = l1_db.get_decision_stats(symbol, hours)
        
        return jsonify({
            'success': True,
            'data': stats,
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_stats: {str(e)}', exc_info=True)
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


@app.route('/api/l1/pipeline/<symbol>')
def get_pipeline_status(symbol):
    """
    è·å–å†³ç­–ç®¡é“çŠ¶æ€ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    
    PR-007: æ”¯æŒä»æ•°æ®åº“è¯»å–å†å²pipeline steps
    
    Args:
        symbol: å¸ç§ç¬¦å·
    
    Query Parameters:
        advisory_id: (å¯é€‰) ä»æ•°æ®åº“è¯»å–æŒ‡å®šå†³ç­–çš„steps
    
    Returns:
        JSON: ç®¡é“æ­¥éª¤è¯¦æƒ…
    """
    try:
        advisory_id = request.args.get('advisory_id', type=int)
        
        if advisory_id:
            # PR-007: ä»æ•°æ®åº“è¯»å–å†å²steps
            steps = l1_db.get_pipeline_steps(advisory_id)
            source = 'database'
        else:
            # è·å–æœ€åä¸€æ¬¡ç®¡é“æ‰§è¡Œçš„æ­¥éª¤è®°å½•ï¼ˆå†…å­˜ï¼‰
            steps = advisory_engine.last_pipeline_steps
            source = 'memory'
        
        if not steps:
            return jsonify({
                'success': True,
                'symbol': symbol,
                'data': {
                    'steps': [],
                    'message': f'æš‚æ— ç®¡é“æ‰§è¡Œè®°å½• (source: {source})',
                    'timestamp': datetime.now().isoformat(),
                    'source': source
                }
            })
        
        return jsonify({
            'success': True,
            'symbol': symbol,
            'data': {
                'steps': steps,
                'timestamp': datetime.now().isoformat(),
                'source': source
            }
        })
    
    except Exception as e:
        logger.error(f"Error getting pipeline status: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/l1/reason-tags/explain')
def get_reason_tag_explanations_api():
    """
    è·å–æ‰€æœ‰reason_tagsçš„ä¸­æ–‡è§£é‡Š
    
    GET /api/l1/reason-tags/explain
    
    Response:
    {
      "success": true,
      "data": {
        "extreme_regime": "æç«¯è¡Œæƒ…ï¼šä»·æ ¼æ³¢åŠ¨è¶…è¿‡é˜ˆå€¼",
        "crowding_risk": "æ‹¥æŒ¤é£é™©ï¼šèµ„é‡‘è´¹ç‡æç«¯ä¸”æŒä»“é‡å¿«é€Ÿå¢é•¿",
        ...
      }
    }
    """
    try:
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        enriched_explanations = {}
        for tag_value, explanation in REASON_TAG_EXPLANATIONS.items():
            from models.reason_tags import ReasonTag
            try:
                tag = ReasonTag(tag_value)
                category = get_reason_tag_category(tag)
                enriched_explanations[tag_value] = {
                    'explanation': explanation,
                    'category': category
                }
            except ValueError:
                enriched_explanations[tag_value] = {
                    'explanation': explanation,
                    'category': 'info'
                }
        
        return jsonify({
            'success': True,
            'data': enriched_explanations,
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_reason_tag_explanations: {str(e)}')
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


@app.route('/api/l1/thresholds', methods=['GET'])
def get_thresholds():
    """
    è·å–å½“å‰é˜ˆå€¼é…ç½®
    
    GET /api/l1/thresholds
    
    Response:
    {
      "success": true,
      "data": {
        "extreme_price_change_1h": 5.0,
        "trend_price_change_6h": 3.0,
        ...
      }
    }
    """
    try:
        return jsonify({
            'success': True,
            'data': advisory_engine.thresholds,
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_thresholds: {str(e)}')
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


@app.route('/api/l1/thresholds', methods=['POST'])
def update_thresholds():
    """
    æ›´æ–°é˜ˆå€¼é…ç½®ï¼ˆéœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰
    
    POST /api/l1/thresholds
    Body:
    {
      "thresholds": {
        "extreme_price_change_1h": 6.0,
        ...
      }
    }
    
    TODO: æ·»åŠ èº«ä»½éªŒè¯
    """
    try:
        new_thresholds = request.json.get('thresholds')
        
        if not new_thresholds:
            return jsonify({
                'success': False,
                'data': None,
                'message': 'Missing thresholds in request body'
            }), 400
        
        advisory_engine.update_thresholds(new_thresholds)
        
        # TODO: ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        
        return jsonify({
            'success': True,
            'data': None,
            'message': f'Thresholds updated: {len(new_thresholds)} items'
        })
    
    except Exception as e:
        logger.error(f'Error in update_thresholds: {str(e)}')
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 400


# ========================================
# APIè·¯ç”± - å¸‚åœºæ•°æ®ï¼ˆä»£ç†ç°æœ‰APIï¼‰
# ========================================

@app.route('/api/markets')
def get_markets():
    """
    è·å–å¯ç”¨å¸‚åœºä¿¡æ¯ï¼ˆPR-010: ä»é…ç½®è¯»å–Symbol Universeï¼‰
    
    Response:
    {
      "success": true,
      "data": {
        "symbols": ["BTC", "ETH", "BNB", "SOL", "XRP"],
        "default_symbol": "BTC",
        "markets": {
          "BTC": {"spot": false, "futures": true},
          "ETH": {"spot": false, "futures": true},
          ...
        }
      }
    }
    """
    try:
        # PR-010: ä»é…ç½®è¯»å–å¸ç§åˆ—è¡¨
        config = advisory_engine.config
        symbol_universe = config.get('symbol_universe', {})
        enabled_symbols = symbol_universe.get('enabled_symbols', ['BTC'])
        default_symbol = symbol_universe.get('default_symbol', 'BTC')
        
        # æ„é€ å¸‚åœºä¿¡æ¯ï¼ˆL1åªæ”¯æŒfuturesï¼‰
        markets = {}
        for symbol in enabled_symbols:
            markets[symbol] = {
                'spot': False,      # L1åªåˆ†æåˆçº¦å¸‚åœº
                'futures': True     # åˆçº¦å¸‚åœº
            }
        
        return jsonify({
            'success': True,
            'data': {
                'symbols': enabled_symbols,
                'default_symbol': default_symbol,
                'markets': markets
            },
            'message': None
        })
    
    except Exception as e:
        logger.error(f'Error in get_markets: {str(e)}')
        return jsonify({
            'success': False,
            'data': None,
            'message': str(e)
        }), 500


# ========================================
# è¾…åŠ©å‡½æ•°
# ========================================

def fetch_market_data(symbol: str, market_type: str = 'futures') -> dict:
    """
    è·å–å¸‚åœºæ•°æ®ï¼ˆä½¿ç”¨çœŸå®Binance APIï¼‰
    
    Args:
        symbol: å¸ç§ç¬¦å·ï¼ˆå¦‚ "BTC"ï¼‰
        market_type: å¸‚åœºç±»å‹ï¼ˆ'futures' æˆ– 'spot'ï¼‰
    
    Returns:
        dict: å¸‚åœºæ•°æ®å­—å…¸ï¼ŒåŒ…å«L1æ‰€éœ€çš„æ‰€æœ‰å­—æ®µ
    """
    try:
        # ä½¿ç”¨Binanceæ•°æ®è·å–å™¨ï¼ˆå·²é›†æˆæ•°æ®ç¼“å­˜ï¼‰
        data = binance_fetcher.fetch_market_data(symbol, market_type)
        
        if data:
            logger.info(f"Fetched market data for {symbol} ({market_type}): price={data['price']:.2f}")
            return data
        else:
            logger.warning(f"Failed to fetch data for {symbol}, using fallback")
            # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›Noneï¼ˆä¸å†ä½¿ç”¨mockæ•°æ®ï¼‰
            return None
    
    except Exception as e:
        logger.error(f"Error fetching market data for {symbol}: {e}", exc_info=True)
        return None


# ========================================
# å¥åº·æ£€æŸ¥
# ========================================

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        'status': 'healthy',
        'service': 'L1 Advisory Layer',
        'timestamp': datetime.now().isoformat()
    })


# ========================================
# é”™è¯¯å¤„ç†
# ========================================

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'success': False,
        'data': None,
        'message': 'Endpoint not found'
    }), 404


@app.errorhandler(500)
def internal_error(error):
    logger.error(f'Internal server error: {error}')
    return jsonify({
        'success': False,
        'data': None,
        'message': 'Internal server error'
    }), 500


# ========================================
# åº”ç”¨å¯åŠ¨
# ========================================

if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("L1 Advisory Layer - Flask Application Starting")
    logger.info("=" * 60)
    logger.info(f"Engine initialized with {len(advisory_engine.thresholds)} thresholds")
    logger.info(f"Database: {l1_db.db_path}")
    logger.info(f"Config watcher: {'Enabled' if config_observer else 'Disabled'}")
    logger.info(f"Scheduler: {'Enabled' if scheduler else 'Disabled'}")
    logger.info("=" * 60)
    
    try:
        # å¯åŠ¨Flaskåº”ç”¨
        # ä½¿ç”¨ä¸åŒç«¯å£é¿å…ä¸æ—§ç‰ˆå†²çª
        app.run(
            host='0.0.0.0', 
            port=5001,  # æ—§ç‰ˆä½¿ç”¨5000ï¼Œæ–°ç‰ˆä½¿ç”¨5001
            debug=True
        )
    finally:
        # åº”ç”¨å…³é—­æ—¶åœæ­¢ç›‘æ§å’Œè°ƒåº¦å™¨
        if config_observer:
            config_observer.stop()
            config_observer.join()
            logger.info("Config watcher stopped")
        
        if scheduler:
            scheduler.shutdown()
            logger.info("Scheduler stopped")
